{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4iplz0qgNN_O"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import nltk\n",
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget 'https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip'\n",
        "# !unzip stanford-ner-2018-10-16.zip"
      ],
      "metadata": {
        "id": "rerYr3i2NR2c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = StanfordNERTagger('filepath/stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
        "                       'filepath/stanford-ner-2018-10-16/stanford-ner.jar',\n",
        "                       encoding='utf-8')"
      ],
      "metadata": {
        "id": "t_T-iDbUQRW8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def page_ner(filepath:str,vol_id):\n",
        "  page_text= open(filepath,errors='replace').read()\n",
        "  tokenized_text = word_tokenize(page_text)\n",
        "  classified_text = st.tag(tokenized_text)\n",
        "  word_lst=[]\n",
        "  #class_lst=[]\n",
        "  for item in classified_text:\n",
        "    #print(type(item))#<class 'tuple'>\n",
        "    aList = list(item)\n",
        "    #word=aList[0]\n",
        "    #cls=aList[-1]\n",
        "    #word_lst.append(word)\n",
        "    #class_lst.append(cls)\n",
        "    word_lst.append(aList)\n",
        "  #ner_df = pd.DataFrame({'word': word_lst,'ner': class_lst})\n",
        "  ner_df = pd.DataFrame(word_lst,columns=[\"word\",\"ner\"])\n",
        "  #select_ner_df = ner_df.loc[ner_df['ner'] != 'O']\n",
        "  #select_ner_df = select_ner_df.drop_duplicates() #deduplicate\n",
        "  page_id= filepath.split('/')[-1].rstrip('.txt')\n",
        "  ner_df['vol_id'] = vol_id\n",
        "  ner_df['page_id'] = page_id\n",
        "\n",
        "  temp_ent_type = None\n",
        "  prev_idx = -1\n",
        "  entity_list = []\n",
        "  start = -1\n",
        "  for x in ner_df[ner_df[\"ner\"]!=\"O\"].to_records():\n",
        "    if (temp_ent_type!=x[\"ner\"]) or (x.index-1!=prev_idx):\n",
        "      if temp_ent_type is not None:\n",
        "        entity_list.append((\" \".join(temp_ent),temp_ent_type,x.vol_id,x.page_id,start,prev_idx+1))\n",
        "      temp_ent_type = x[\"ner\"]\n",
        "      temp_ent = [x[\"word\"]]\n",
        "      start = x.index\n",
        "    else:\n",
        "      temp_ent = temp_ent + [x[\"word\"]]\n",
        "    prev_idx = x.index\n",
        "\n",
        "  # last entity\n",
        "  if temp_ent_type is not None:\n",
        "    entity_list.append((\" \".join(temp_ent),temp_ent_type,x.vol_id,x.page_id,start,prev_idx+1))\n",
        "  \n",
        "  ner_df = pd.DataFrame(entity_list,columns = [\"word\",\"ner\",\"vol_id\",\"page_id\",\"start_token\",\"end_token\"])\n",
        "  return ner_df"
      ],
      "metadata": {
        "id": "YGO2SbH4NRze"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_workset(workset_path:str):\n",
        "   df_lst=[]\n",
        "   vol_paths = Path(workset_path).glob('*')\n",
        "   for vol_path in vol_paths:\n",
        "      print(vol_path)\n",
        "      # print(type(vol_path)) # <class 'pathlib.PosixPath'>\n",
        "      vol_path_str=vol_path.as_posix()#convert to str\n",
        "      vol_id=vol_path_str.split('/')[-1]\n",
        "      print(vol_id)\n",
        "      os.chdir(vol_path)\n",
        "      extension = 'txt'\n",
        "      all_pages = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "      for page in all_pages:\n",
        "        filepath=vol_path_str+'/'+page #fullpath\n",
        "        #print(filepath)\n",
        "        sub_df=page_ner(filepath,vol_id)\n",
        "        df_lst.append(sub_df)\n",
        "        #print(len(df_lst))\n",
        "   df_combined=pd.concat(df_lst) \n",
        "   return df_combined\n"
      ],
      "metadata": {
        "id": "KhAvNrhiPdqj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = process_workset('filepath/testing_volume/')\n",
        "\n",
        "test_df.to_csv('filepath/outcomes.csv')"
      ],
      "metadata": {
        "id": "4VRsHs0-PKBk"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}